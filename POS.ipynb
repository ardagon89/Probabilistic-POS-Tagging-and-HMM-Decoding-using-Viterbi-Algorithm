{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "o0VvgY_KiT0O",
    "outputId": "c46a76ac-9141-470a-9d0f-fc3e88109a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       will                   Janet                    back                     the                    bill\n",
      "NNP                     0.0          4.73750016e-12                     0.0  1.4537573338978714e-22                     0.0\n",
      " MD  0.00018505859999999998                     0.0                     0.0                     0.0                     0.0\n",
      " VB                8.68e-08                     0.0  2.8652400967679996e-18                     0.0   6.046454487031505e-26\n",
      " JJ                     0.0                     0.0      1.353030045696e-17                     0.0                     0.0\n",
      " NN                8.98e-06                     0.0   6.169741208371199e-17                     0.0  1.1970606475374611e-20\n",
      " RB                     0.0                     0.0     4.4539134004224e-16                     0.0                     0.0\n",
      " DT                     0.0                     0.0                     0.0  1.0797240155413401e-17                     0.0\n",
      "The probability of the below POS sequence is : 1.1970606475374611e-20\n",
      "will_MD Janet_NNP back_RB the_DT bill_NN\n",
      "C:\\Users\\shari\\AppData\\Roaming\\jupyter\\runtime\\kernel-17d2314f-284c-4d05-ade0-88852f889159.json\n",
      "('That', 'DT')  0.0033402258628916813    \n",
      "('DT', '<s>')   0.2693693693693694       \n",
      "('may', 'MD')   0.04819277108433735      \n",
      "('MD', 'DT')    0.0014315253698107206    \n",
      "('be', 'VB')    0.12412412412412413      \n",
      "('VB', 'MD')    0.844578313253012        \n",
      "('cold', 'JJ')  0.0020655822359927703    \n",
      "('JJ', 'VB')    0.053553553553553554     \n",
      "('comfort', 'NN') 0.0001801477211313277    \n",
      "('NN', 'JJ')    0.5435063258455978       \n",
      "('for', 'IN')   0.06795469686875416      \n",
      "('IN', 'NN')    0.2673392181588903       \n",
      "('Belle', 'NNP') 0.0001336005344021376    \n",
      "('NNP', 'IN')   0.21092604930046635      \n",
      "('McFall', 'NNP') 0.0001336005344021376    \n",
      "('NNP', 'NNP')  0.2936539746158985       \n",
      "('and', 'CC')   0.7965186074429772       \n",
      "('CC', 'NNP')   0.03674014696058784      \n",
      "('350', 'CD')   0.004222972972972973     \n",
      "('CD', 'CC')    0.02220888355342137      \n",
      "('other', 'JJ') 0.019106635682933127     \n",
      "('JJ', 'CD')    0.048564189189189186     \n",
      "('workers', 'NNS') 0.03043361163479666      \n",
      "('NNS', 'JJ')   0.17764007229537826      \n",
      "('who', 'WP')   0.6687116564417178       \n",
      "('WP', 'NNS')   0.00915701589011581      \n",
      "('in', 'IN')    0.20599600266489007      \n",
      "('IN', 'WP')    0.012269938650306749     \n",
      "('October', 'NNP') 0.0025384101536406146    \n",
      "('NNP', 'IN')   0.21092604930046635      \n",
      "('lost', 'VBD') 0.0035072336694432268    \n",
      "('VBD', 'NNP')  0.07201068804275217      \n",
      "('their', 'PRP$') 0.13700787401574804      \n",
      "('PRP$', 'VBD') 0.018851380973257344     \n",
      "('jobs', 'NNS') 0.024239159709130084     \n",
      "('NNS', 'PRP$') 0.15433070866141732      \n",
      "('at', 'IN')    0.07581612258494337      \n",
      "('IN', 'NNS')   0.29625639644492324      \n",
      "('the', 'DT')   0.4956258947033561       \n",
      "('DT', 'IN')    0.34950033311125916      \n",
      "('Cedartown', 'NNP') 0.0001336005344021376    \n",
      "('NNP', 'DT')   0.10529664386829966      \n",
      "('plant', 'NN') 0.2080706179066835       \n",
      "('NN', 'NNP')   0.08710754843019372      \n",
      "('owned', 'VBN') 0.013024602026049204     \n",
      "('VBN', 'NN')   0.009457755359394703     \n",
      "('by', 'IN')    0.038507661558960696     \n",
      "('IN', 'VBN')   0.3719247467438495       \n",
      "('Arrow', 'NNP') 0.0002672010688042752    \n",
      "('NNP', 'IN')   0.21092604930046635      \n",
      "('Shirt', 'NNP') 0.0002672010688042752    \n",
      "('NNP', 'NNP')  0.2936539746158985       \n",
      "(',', ',')      0.998959958398336        \n",
      "(',', 'NNP')    0.1967935871743487       \n",
      "('a', 'DT')     0.2767615714967393       \n",
      "('DT', ',')     0.12610504420176807      \n",
      "('unit', 'NN')  0.004053323725454873     \n",
      "('NN', 'DT')    0.5094639732781931       \n",
      "('of', 'IN')    0.2078614257161892       \n",
      "('IN', 'NN')    0.2673392181588903       \n",
      "('Bidermann', 'NNP') 0.0001336005344021376    \n",
      "('NNP', 'IN')   0.21092604930046635      \n",
      "('International', 'NNP') 0.0037408149632598532    \n",
      "('NNP', 'NNP')  0.2936539746158985       \n",
      "('NNP', '</s>') 0                        \n",
      "That_DT may_MD be_VB cold_JJ comfort_NN for_IN Belle_NNP McFall_NNP and_CC 350_CD other_JJ workers_NNS who_WP in_IN October_NNP lost_VBD their_PRP$ jobs_NNS at_IN the_DT Cedartown_NNP plant_NN owned_VBN by_IN Arrow_NNP Shirt_NNP ,_, a_DT unit_NN of_IN Bidermann_NNP International_NNP\n",
      "The probability of the above sentence is : 0\n"
     ]
    }
   ],
   "source": [
    "def get_uni_bi_grams(filename):\n",
    "    file1 = open(filename,\"r\")\n",
    "    tagslist = []\n",
    "    wordslist = []\n",
    "    word_has_tags = {}\n",
    "    for line in file1.readlines():\n",
    "        token_arr = line.split()\n",
    "        tagslist.append(['<s>']+[token.split('_')[1] for token in token_arr]+['</s>'])\n",
    "        wordslist.append([token.split('_')[0] for token in token_arr])\n",
    "        for token in token_arr:\n",
    "            word_tag = token.split('_')\n",
    "            if word_tag[0] in word_has_tags:\n",
    "                if word_tag[1] not in word_has_tags[word_tag[0]]:\n",
    "                    word_has_tags[word_tag[0]].append(word_tag[1])\n",
    "            else:\n",
    "                word_has_tags[word_tag[0]] = [word_tag[1]]\n",
    "\n",
    "    return wordslist, tagslist, word_has_tags\n",
    "\n",
    "def get_unigram_count(wordslist):\n",
    "    unigram_count = {}\n",
    "    total_tokens = 0\n",
    "    for line in wordslist:\n",
    "        for word in line:\n",
    "            if word not in unigram_count:\n",
    "                unigram_count[word] = 1\n",
    "            else:\n",
    "                unigram_count[word] += 1\n",
    "        total_tokens += len(line)\n",
    "        \n",
    "    return unigram_count, total_tokens\n",
    "\n",
    "def get_WT_bigram_count(wordslist, tagslist):\n",
    "    tags_bigram = {}\n",
    "    word_tag_bigram = {}\n",
    "    for line in tagslist:\n",
    "        for i in range(1, len(line)):\n",
    "            if (line[i-1], line[i]) not in tags_bigram:\n",
    "                tags_bigram[(line[i-1], line[i])] = 1\n",
    "            else:\n",
    "                tags_bigram[(line[i-1], line[i])] += 1\n",
    "                \n",
    "    for line in range(len(wordslist)):\n",
    "        for i in range(len(wordslist[line])):\n",
    "            if (wordslist[line][i], tagslist[line][i+1]) not in word_tag_bigram:\n",
    "                word_tag_bigram[(wordslist[line][i], tagslist[line][i+1])] = 1\n",
    "            else:\n",
    "                word_tag_bigram[(wordslist[line][i], tagslist[line][i+1])] += 1\n",
    "    \n",
    "    return tags_bigram, word_tag_bigram\n",
    "\n",
    "def get_WT_bigram_prob(tags_bigram, word_tag_bigram, unigram_count):\n",
    "    tags_prob = {}\n",
    "    word_tag_prob = {}\n",
    "    \n",
    "    for key in tags_bigram:\n",
    "        tags_prob[key] = tags_bigram[key]/unigram_count[key[0]]\n",
    "        \n",
    "    for key in word_tag_bigram:\n",
    "        word_tag_prob[key] = word_tag_bigram[key]/unigram_count[key[1]]\n",
    "        \n",
    "    return tags_prob, word_tag_prob\n",
    "\n",
    "def find_sentence_prob(sentence, tags_prob, word_tag_prob, word_has_tags, probs, prev_tag, result):\n",
    "    while len(sentence)>0 and len(word_has_tags[sentence[0]]) == 1:\n",
    "        result.append(sentence[0]+'_'+word_has_tags[sentence[0]][0])\n",
    "        if (sentence[0],word_has_tags[sentence[0]][0]) in word_tag_prob and (prev_tag, word_has_tags[sentence[0]][0]) in tags_prob:\n",
    "            probs *= word_tag_prob[(sentence[0],word_has_tags[sentence[0]][0])]\n",
    "            probs *= tags_prob[(prev_tag, word_has_tags[sentence[0]][0])]\n",
    "        else:\n",
    "            probs = 0\n",
    "        prev_tag = word_has_tags[sentence[0]][0]\n",
    "        sentence = sentence[1:]\n",
    "    if len(sentence)==0:\n",
    "        if (prev_tag, '</s>') in tags_prob:\n",
    "            probs *= tags_prob[(prev_tag, '</s>')]\n",
    "        else:\n",
    "            probs = 0\n",
    "        return probs, result\n",
    "    else:\n",
    "        max_prob = -1\n",
    "        for cur_tag in word_has_tags[sentence[0]]:\n",
    "            if (sentence[0],cur_tag) in word_tag_prob and (prev_tag, cur_tag) in tags_prob:\n",
    "                temp_prob, temp_result = find_sentence_prob(sentence[1:], tags_prob, word_tag_prob, word_has_tags, \n",
    "                                               probs*word_tag_prob[(sentence[0],cur_tag)]*tags_prob[(prev_tag, cur_tag)],\n",
    "                                               cur_tag, result+[sentence[0]+'_'+cur_tag])\n",
    "                if temp_prob > max_prob:\n",
    "                    max_prob = temp_prob\n",
    "                    new_result  = temp_result\n",
    "        return max_prob, new_result\n",
    "        \n",
    "def get_prob_table(filename):\n",
    "    result = {}\n",
    "    file1 = open(filename,\"r\")\n",
    "    for i, line in enumerate(file1.readlines()):\n",
    "        if i==0:\n",
    "            tags = line.rstrip().split(',')\n",
    "        else:\n",
    "            for j, word in enumerate(line.rstrip().split(',')):\n",
    "                if j==0:\n",
    "                    prev = word\n",
    "                else:\n",
    "                    result[(prev,tags[j])] = float(word)\n",
    "    return result, tags[1:]\n",
    "\n",
    "def get_sentence_prob(sentence, tags, trans_tab, obser_tab):\n",
    "    calc_tab = []\n",
    "    for word in sentence:\n",
    "        col = []\n",
    "        for tag in tags:\n",
    "            if len(calc_tab)==0:\n",
    "                col.append([1*trans_tab[('<s>',tag)]*obser_tab[(tag,word)], -1, '<s>', tag, word])\n",
    "            else:\n",
    "                max_prob = -1\n",
    "                cell = []\n",
    "                for x, item in enumerate(calc_tab[-1]):\n",
    "                    temp_prob = item[0]*trans_tab[(tags[x],tag)]*obser_tab[(tag,word)]\n",
    "                    if temp_prob > max_prob:\n",
    "                        max_prob = temp_prob\n",
    "                        cell = [temp_prob, x, tags[x], tag, word]\n",
    "                col.append(cell)\n",
    "        calc_tab.append(col)\n",
    "    \n",
    "    print(''.rjust(3), end='')\n",
    "    for y in sentence:\n",
    "        print(y.rjust(24), end='')\n",
    "    print()\n",
    "    for x,tag in enumerate(tags):\n",
    "        print(tag.rjust(3), end='')\n",
    "        for row in calc_tab:\n",
    "            print(str(row[x][0]).rjust(24), end='')\n",
    "        print()\n",
    "        \n",
    "    max_index = np.argmax([subrow[0] for subrow in calc_tab[-1]])\n",
    "    print('The probability of the below POS sequence is :', calc_tab[-1][max_index][0])\n",
    "    result = []\n",
    "    print_arr = []\n",
    "    for row in reversed(calc_tab):\n",
    "        result.append(row[max_index][4]+'_'+row[max_index][3])\n",
    "        max_index = row[max_index][1]\n",
    "    return result[::-1]\n",
    "\n",
    "def print_probs(tags, tags_prob, word_tag_prob):\n",
    "    prev = '<s>'\n",
    "    for token in tags:\n",
    "        word, tag = token.split('_')\n",
    "        if (word, tag) in word_tag_prob:\n",
    "            print(str((word, tag)).ljust(15), str(word_tag_prob[(word, tag)]).ljust(25))\n",
    "        else:\n",
    "            print(str((word, tag)).ljust(15), str(0).ljust(25))\n",
    "        if (prev, tag) in tags_prob:\n",
    "            print(str((tag, prev)).ljust(15), str(tags_prob[(prev, tag)]).ljust(25))\n",
    "        else:\n",
    "            print(str((tag, prev)).ljust(15), str(0).ljust(25))\n",
    "        prev = tag\n",
    "    if (prev, '</s>') in tags_prob:\n",
    "        print(str((prev, '</s>')).ljust(15), str(tags_prob[(prev, '</s>')]).ljust(25))\n",
    "    else:\n",
    "        print(str((prev, '</s>')).ljust(15), str(0).ljust(25))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    trans_tab, tags = get_prob_table('Transition_Table.csv')\n",
    "    obser_tab, _ = get_prob_table('Observation_Table.csv')\n",
    "    sentence = \"will Janet back the bill\".split()\n",
    "    POS_tags = get_sentence_prob(sentence, tags, trans_tab, obser_tab)\n",
    "    print(' '.join(POS_tags))\n",
    "    print((sys.argv[2]))\n",
    "    wordslist, tagslist, word_has_tags = get_uni_bi_grams(\"NLP6320_POSTaggedTrainingSet-Windows.txt\")\n",
    "    unigram_count, total_tokens = get_unigram_count(tagslist)\n",
    "    tags_bigram, word_tag_bigram = get_WT_bigram_count(wordslist, tagslist)\n",
    "    tags_prob, word_tag_prob = get_WT_bigram_prob(tags_bigram, word_tag_bigram, unigram_count)\n",
    "    sentence  = \"That may be cold comfort for Belle McFall and 350 other workers who in October lost their jobs at the Cedartown plant owned by Arrow Shirt , a unit of Bidermann International\".split()\n",
    "    prob, tags = find_sentence_prob(sentence, tags_prob, word_tag_prob, word_has_tags, 1, '<s>', [])\n",
    "    print_probs(tags, tags_prob, word_tag_prob)\n",
    "    print(' '.join(tags))\n",
    "    print('The probability of the above sentence is :',prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3j9HiQxvaIe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_count 7\n",
      "N_c 184\n",
      "N_c 205\n",
      "N_c 16520\n",
      "1813\n",
      "1363\n",
      "4.520905085529811e-05\n",
      "3.0071704357418643\n"
     ]
    }
   ],
   "source": [
    "print('bigram_count',bigram_count[('to','work')])\n",
    "print('N_c', len(N_c[8]))\n",
    "print('N_c', len(N_c[7]))\n",
    "print('N_c', len(N_c[1]))\n",
    "print(len(N_c[3]))\n",
    "print(len(N_c[4]))\n",
    "print(p_star[3])\n",
    "print(c_star[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_has_tags['International']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bigram.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
